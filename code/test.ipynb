{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_training.ipynb      learningBase\t\t       test.ipynb\n",
      "ai_training_ols.ipynb  learningBase_ols\t\t       tf_model_1.h5\n",
      "apply_annSolution.py   model.ipynb\t\t       tf_model_1.keras\n",
      "housing_data.csv       scraping_and_preparation.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls ../AI-CPS/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../AI-CPS/code/sarcasm_model.h5\n",
      "Loading activation data from: ../AI-CPS/code/activation_data.csv\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Tweet: Overheard as my 13 year old games with a friend: 'You smell like tartare sauce!' #MontyPythonesqueDisses\n",
      "Prediction: Not Sarcastic (score: 0.4801)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIG: Update paths as needed\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_PATH = \"../AI-CPS/code/sarcasm_model.h5\"\n",
    "ACTIVATION_PATH = \"../AI-CPS/code/activation_data.csv\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Hyperparameters for vectorization (must match what you used in training)\n",
    "# ------------------------------------------------------------------------------\n",
    "MAX_TOKENS = 1000\n",
    "SEQUENCE_LENGTH = 50\n",
    "\n",
    "def load_model(model_path: str):\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    # Load without compiling to avoid deserializing the metric functions\n",
    "    return tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "def load_activation_data(csv_path: str):\n",
    "    print(f\"Loading activation data from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "def predict_inference(model, df):\n",
    "    \"\"\"\n",
    "    1) Adapt TextVectorization on the single activation file itself.\n",
    "    2) Vectorize those tweets to integer sequences.\n",
    "    3) Model expects integer sequences (from its Embedding layer).\n",
    "    4) Print predictions.\n",
    "    \"\"\"\n",
    "    tweets = df[\"tweet\"].astype(str).values\n",
    "\n",
    "    # Build a vectorizer that matches your training parameters\n",
    "    vectorizer = layers.TextVectorization(\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=SEQUENCE_LENGTH\n",
    "    )\n",
    "\n",
    "    # WARNING: Adapting on activation_data.csv alone means the vocabulary\n",
    "    #          won't match full training. This is purely a demonstration.\n",
    "    vectorizer.adapt(tweets)\n",
    "\n",
    "    # Convert raw text to integer sequences\n",
    "    tweets_vec = vectorizer(tweets)\n",
    "\n",
    "    # Model expects integer sequences\n",
    "    predictions = model.predict(tweets_vec)\n",
    "\n",
    "    # Print predictions\n",
    "    for i, pred in enumerate(predictions):\n",
    "        label = \"Sarcastic\" if pred[0] >= 0.5 else \"Not Sarcastic\"\n",
    "        print(f\"Tweet: {tweets[i]}\")\n",
    "        print(f\"Prediction: {label} (score: {pred[0]:.4f})\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = load_model(MODEL_PATH)\n",
    "    df_act = load_activation_data(ACTIVATION_PATH)\n",
    "    predict_inference(model, df_act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../AI-CPS/code/learningBase/sarcasm_model.keras\n",
      "  Length      Date    Time    Name\n",
      "---------  ---------- -----   ----\n",
      "       64  01-01-1980 00:00   metadata.json\n",
      "     3735  01-01-1980 00:00   config.json\n",
      "  1121692  02-01-2025 14:16   model.weights.h5\n",
      "---------                     -------\n",
      "  1125491                     3 files\n"
     ]
    }
   ],
   "source": [
    "!unzip -l ../AI-CPS/code/learningBase/sarcasm_model.keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data\t\t\t\t    OLS_model\t\t\t      'week 4'\n",
      " dataset02_testing.csv\t\t    requirements.txt\t\t      'week 5'\n",
      " dataset02_training.csv\t\t    scatter_with_regression_line.pdf  'week 6'\n",
      "'FahrzeuguÌˆbersicht 2024 (2).xlsx'   tf_model_1.keras\t\t      'week 7'\n",
      " housing_data.csv\t\t    UE_04_App2_BoxPlot.pdf\t      'week 8'\n",
      " intro_pytorch.ipynb\t\t    Untitled.ipynb\t\t      'week 9'\n",
      " learningBase\t\t\t   'week 2'\n",
      " ml2_python3_11\t\t\t   'week 3'\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OLS model from: /tmp/knowledgeBase/currentOlsSoluGon.pickle\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/knowledgeBase/currentOlsSoluGon.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[267], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     ols_results \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     df_act \u001b[38;5;241m=\u001b[39m load_activation_data(ACTIVATION_PATH)\n\u001b[1;32m     76\u001b[0m     predict_inference(ols_results, df_act)\n",
      "Cell \u001b[0;32mIn[267], line 26\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading OLS model from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Use statsmodels' load function (this assumes your model was saved via results.save(...))\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/statsmodels/iolib/smpickle.py:41\u001b[0m, in \u001b[0;36mload_pickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03mLoad a previously saved object\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mThis method can be used to load *both* models and results.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_file_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/statsmodels/iolib/openfile.py:65\u001b[0m, in \u001b[0;36mget_file_obj\u001b[0;34m(fname, mode, encoding)\u001b[0m\n\u001b[1;32m     63\u001b[0m     fname \u001b[38;5;241m=\u001b[39m Path(fname)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, Path):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fname\u001b[38;5;241m.\u001b[39mopen(mode\u001b[38;5;241m=\u001b[39mmode, encoding\u001b[38;5;241m=\u001b[39mencoding)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/knowledgeBase/currentOlsSoluGon.pickle'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Paths INSIDE the container:\n",
    "# We expect the activation data is copied to /tmp/activationBase\n",
    "# We expect the OLS model is copied to /tmp/knowledgeBase\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_PATH = \"../AI-CPS/code/currentOlsSoluGon.pickle\"\n",
    "ACTIVATION_PATH = \"../AI-CPS/code/activation_data.csv\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Hyperparameters for vectorization (must match what you used in training)\n",
    "# ------------------------------------------------------------------------------\n",
    "MAX_TOKENS = 1000\n",
    "SEQUENCE_LENGTH = 50\n",
    "\n",
    "def load_model(model_path: str):\n",
    "    print(f\"Loading OLS model from: {model_path}\")\n",
    "    # Use statsmodels' load function (this assumes your model was saved via results.save(...))\n",
    "    return sm.load(model_path)\n",
    "\n",
    "def load_activation_data(csv_path: str):\n",
    "    print(f\"Loading activation data from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "def predict_inference(ols_results, df):\n",
    "    \"\"\"\n",
    "    For each tweet in the activation data:\n",
    "      1. Adapt a TextVectorization layer on the tweets (for demonstration).\n",
    "      2. Convert the raw tweet strings to integer sequences.\n",
    "      3. Add a constant column (to match the training data).\n",
    "      4. Use the loaded OLS model to predict.\n",
    "      5. Print the predictions.\n",
    "    \"\"\"\n",
    "    tweets = df[\"tweet\"].astype(str).values\n",
    "\n",
    "    # Build a TextVectorization layer (this should match the training configuration)\n",
    "    vectorizer = layers.TextVectorization(\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=SEQUENCE_LENGTH\n",
    "    )\n",
    "    \n",
    "    # WARNING: Adapting on the activation data alone means the vocabulary\n",
    "    # will likely differ from training. This is acceptable here for demonstration.\n",
    "    vectorizer.adapt(tweets)\n",
    "    \n",
    "    # Convert the raw text tweets into integer sequences\n",
    "    tweets_tensor = vectorizer(tweets)  # This returns a tf.Tensor\n",
    "    tweets_int = tweets_tensor.numpy()   # Convert to a NumPy array\n",
    "\n",
    "    # In training, you added a constant column to the integer sequences.\n",
    "    # Do the same here so the input matches what the OLS model expects.\n",
    "    X_activation = sm.add_constant(tweets_int, has_constant=\"add\")\n",
    "    \n",
    "    # Predict using the loaded OLS model\n",
    "    predictions = ols_results.predict(X_activation)\n",
    "    \n",
    "    # Print predictions\n",
    "    for i, pred in enumerate(predictions):\n",
    "        # For demonstration, we threshold at 0.5\n",
    "        label = \"Sarcastic\" if pred >= 0.5 else \"Not Sarcastic\"\n",
    "        print(f\"Tweet: {tweets[i]}\")\n",
    "        print(f\"Prediction: {label} (score: {pred:.4f})\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ols_results = load_model(MODEL_PATH)\n",
    "    df_act = load_activation_data(ACTIVATION_PATH)\n",
    "    predict_inference(ols_results, df_act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ai-cps",
   "language": "python",
   "name": "venv-ai-cps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
